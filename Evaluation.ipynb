{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from lib.DataManager import *\n",
    "from lib.PolicyStats import *\n",
    "import os\n",
    "import cma\n",
    "from cma.constraints_handler import AugmentedLagrangian, PopulationEvaluator\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE CHANGE THE NUM_STATES FOR GRIDWORLD / REAL\n",
    "# num_states = 18\n",
    "num_states = 23\n",
    "num_actions = 4\n",
    "gamma = 0.95\n",
    "delta = 0.01 #1 - delta, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = GetHistories(\"gridworld_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_exploratory_J = 0\n",
    "for traj in histories:\n",
    "    rewards = traj[:,2]\n",
    "    nonzero_idx = np.where(rewards != 0)[0]\n",
    "    for idx in nonzero_idx:\n",
    "        avg_exploratory_J += rewards[idx] * (gamma ** idx)\n",
    "avg_exploratory_J /= len(histories)\n",
    "print(avg_exploratory_J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = int(len(histories) * .8)\n",
    "train = histories[:split_idx]\n",
    "test = histories[split_idx:]\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Exploration Policy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_policy = GetPolicy(train, num_states, num_actions, 10)\n",
    "print(exploration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pick Importance Sampling Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISFunc = ImportanceSampling\n",
    "ISFunc = PDImportanceSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluate Current Policy On Candidate/Safety Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(Je > J_lower_bound) > 1 - delta\n",
    "J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, exploration_policy, ISFunc, delta, len(test))\n",
    "print(J_predicted_lower_bound)\n",
    "J_safety_lower_bound = Safety_Test(test, exploration_policy, gamma, exploration_policy, ISFunc, delta)\n",
    "print(J_safety_lower_bound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Helper Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_softmax(policy):\n",
    "    numerators = np.exp(policy)\n",
    "    return (numerators.T / np.sum(numerators, axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_explore():\n",
    "    best_policy = exploration_policy.copy()\n",
    "    max_lower_bound = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        random_step = np.random.normal(0, 1, best_policy.shape)\n",
    "        new_policy = policy_softmax(best_policy + random_step)\n",
    "\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test))\n",
    "        print(\"Predicted Lower Bound: \", J_predicted_lower_bound)\n",
    "        if(J_predicted_lower_bound > max_lower_bound):\n",
    "            print(\"Policy Updated\")\n",
    "            best_policy = new_policy\n",
    "            max_lower_bound = J_predicted_lower_bound\n",
    "        print(\"---------------\")\n",
    "\n",
    "    print(best_policy)\n",
    "    return best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unconstrained_explore():\n",
    "    def objective(s):\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        print(avgIS)\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while not es.stop():\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [objective(s) for s in solutions])\n",
    "        \n",
    "    return policy_softmax(es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_barrier_constrained_explore(lower_bound_goal, max_updates=100):\n",
    "    def constraint(new_policy, avgIS):\n",
    "        EPSILON = 0.01\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test), avgIS)\n",
    "        return 1 / (max(J_predicted_lower_bound - lower_bound_goal, EPSILON))\n",
    "    \n",
    "    def objective(new_policy, avgIS):\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    def optimizing_function(s):\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        objective_score = objective(new_policy, avgIS)\n",
    "        constraint_score = constraint(new_policy, avgIS)\n",
    "        score = objective_score + constraint_score\n",
    "        print(\"score : \" + str(score) + \"\\n---constraint_score : \" + str(constraint_score) + \"\\n---objective_score : \" + str(objective_score))\n",
    "        return score\n",
    "    \n",
    "    i = 0\n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while (not es.stop() and i != max_updates):\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(\"Update : \" + str(i))\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [optimizing_function(s) for s in solutions])\n",
    "        i += 1\n",
    "        \n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explore Policies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_performance = 1.41537 #from paper\n",
    "target_performance = avg_exploratory_J\n",
    "trained_es = inv_barrier_constrained_explore(target_performance + 0.1, 50)\n",
    "new_policy = policy_softmax(trained_es.ask()[0].reshape(num_states, num_actions)), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ES_Convergence : \" + str(sum(trained_es.mean**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Final Results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test))\n",
    "print(J_predicted_lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_safety_lower_bound = Safety_Test(test, exploration_policy, gamma, new_policy, ISFunc, delta)\n",
    "print(J_safety_lower_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"policies\\\\delta_\" + str(delta) + \"\\\\\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "np.save(folder_name + \"safety_\" + str(J_safety_lower_bound), new_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(new_policy,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"gridworld\" + str(J_safety_lower_bound), new_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
