{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from lib.DataManager import *\n",
    "from lib.PolicyStats import *\n",
    "import os\n",
    "import cma\n",
    "from cma.constraints_handler import AugmentedLagrangian, PopulationEvaluator\n",
    "from IPython import display\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET PARAMS---\n",
    "USE_GRIDWORLD = True\n",
    "USE_PDIS = True\n",
    "num_train_intervals = 40\n",
    "percent_increase = 0.1\n",
    "\n",
    "num_states = 18\n",
    "if(USE_GRIDWORLD):\n",
    "    num_states = 23\n",
    "num_actions = 4\n",
    "gamma = 0.95\n",
    "delta = 0.01 #1 - delta, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 0\n",
      "line 1000000\n",
      "line 2000000\n",
      "line 3000000\n",
      "line 4000000\n",
      "line 5000000\n",
      "line 6000000\n",
      "line 7000000\n",
      "-0.4547553405083997\n"
     ]
    }
   ],
   "source": [
    "path = \"data\\data.csv\"\n",
    "if(USE_GRIDWORLD):\n",
    "    path = \"data\\gridworld_data.csv\"\n",
    "\n",
    "histories = GetHistories(path, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Baseline Return : -0.9422074904303682\n"
     ]
    }
   ],
   "source": [
    "avg_exploratory_J = GetAverageReturn(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set Target***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Performance : -0.8479867413873314\n"
     ]
    }
   ],
   "source": [
    "target_performance = GetTargetPerformance(USE_GRIDWORLD, avg_exploratory_J, percent_increase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Split Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "train, test = SplitData(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Exploration Policy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "exploration_policy = GetPolicy(train, num_states, num_actions, 1000)\n",
    "print(exploration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pick Importance Sampling Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISFunc = ImportanceSampling\n",
    "if(USE_PDIS):\n",
    "    ISFunc = PDImportanceSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluate Current Policy On Candidate/Safety Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: -0.9422074904303682\n",
      "Predicted Baseline: -1.182750951767137\n",
      "Safety Baseline: 0.4322671551878296\n",
      "---distance of return from lower bounds ---\n",
      "Looseness Of Prediction : 0.24054346133676885\n",
      "Looseness Of Safety : -1.3744746456181978\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Lower Bound Greater Than Average Return!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a7ed8a18f903>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"policies/gw/delta_0.01/safety_0.4380582971385134.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mConfirmBounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_exploratory_J\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexploration_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mISFunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\School\\F20\\687\\rl-policy-improvement\\lib\\PolicyStats.py\u001b[0m in \u001b[0;36mConfirmBounds\u001b[1;34m(is_lower_bounded, value, train, test, exploration_policy, gamma, new_policy, ISFunc, delta)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbl_pred_looseness\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbl_safety_looseness\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Lower Bound Greater Than Average Return!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Lower Bound Greater Than Average Return!"
     ]
    }
   ],
   "source": [
    "ConfirmBounds(True, avg_exploratory_J, train, test, exploration_policy, gamma, exploration_policy, ISFunc, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Helper Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_softmax(policy):\n",
    "    numerators = np.exp(policy)\n",
    "    return (numerators.T / np.sum(numerators, axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sucks, never use it lol\n",
    "def random_explore():\n",
    "    best_policy = exploration_policy.copy()\n",
    "    max_lower_bound = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        random_step = np.random.normal(0, 1, best_policy.shape)\n",
    "        new_policy = policy_softmax(best_policy + random_step)\n",
    "\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test))\n",
    "        print(\"Predicted Lower Bound: \", J_predicted_lower_bound)\n",
    "        if(J_predicted_lower_bound > max_lower_bound):\n",
    "            print(\"Policy Updated\")\n",
    "            best_policy = new_policy\n",
    "            max_lower_bound = J_predicted_lower_bound\n",
    "        print(\"---------------\")\n",
    "\n",
    "    print(best_policy)\n",
    "    return best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also sucks, use the constrained variant\n",
    "def unconstrained_explore():\n",
    "    def objective(s):\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        print(avgIS)\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while not es.stop():\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [objective(s) for s in solutions])\n",
    "        \n",
    "    return policy_softmax(es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_barrier_constrained_explore(lower_bound_goal, max_updates=10):\n",
    "    #Helper Functions\n",
    "    #This constraint makes sure our results are passing the safety prediction test\n",
    "    def constraint(new_policy, avgIS):\n",
    "        EPSILON = 0.001 #determines penalty for failing lower bound test\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test), avgIS)\n",
    "        return 1 / (max(J_predicted_lower_bound - lower_bound_goal, EPSILON)) #TODO validate constraint\n",
    "    \n",
    "    #This objective results in maximizing the average importance sampling\n",
    "    def objective(new_policy, avgIS):\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    def optimizing_function(s):\n",
    "        #softmax generated policy\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        \n",
    "        #caches the averageIS so we don't have to recompute\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        \n",
    "        #computes score from the objective and constraint\n",
    "        objective_score = objective(new_policy, avgIS)\n",
    "        constraint_score = constraint(new_policy, avgIS)\n",
    "        score = objective_score + constraint_score\n",
    "        print(\"score : \" + str(score) + \"\\n---constraint_score : \" + str(constraint_score) + \"\\n---objective_score : \" + str(objective_score))\n",
    "        return score\n",
    "    \n",
    "    i = 0\n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while (not es.stop() and i != max_updates):\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(\"Update : \" + str(i))\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [optimizing_function(s) for s in solutions])\n",
    "        i += 1\n",
    "        \n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explore Policies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update : 39\n",
      "[[0.1411552  0.32763659 0.14267479 0.38853341]\n",
      " [0.29040004 0.19519507 0.37528888 0.13911602]\n",
      " [0.18865509 0.13448424 0.47577341 0.20108727]\n",
      " [0.09621078 0.13973157 0.72538069 0.03867697]\n",
      " [0.10482291 0.02555334 0.29842402 0.57119974]\n",
      " [0.06412993 0.55789676 0.23723216 0.14074116]\n",
      " [0.005124   0.93733947 0.02520555 0.03233098]\n",
      " [0.08643774 0.43526381 0.382331   0.09596746]\n",
      " [0.04675187 0.05109971 0.77962925 0.12251917]\n",
      " [0.15201966 0.22511192 0.32056988 0.30229854]\n",
      " [0.74563201 0.04768682 0.07138929 0.13529189]\n",
      " [0.0703789  0.26940693 0.05041009 0.60980408]\n",
      " [0.24291501 0.25638222 0.23439284 0.26630993]\n",
      " [0.20042386 0.08997213 0.64603063 0.06357338]\n",
      " [0.27018119 0.32188015 0.02036831 0.38757035]\n",
      " [0.93287011 0.05237133 0.00325309 0.01150548]\n",
      " [0.09475624 0.15403891 0.22790837 0.52329648]\n",
      " [0.00665665 0.41395282 0.46927094 0.11011958]\n",
      " [0.04969172 0.10690393 0.8090994  0.03430495]\n",
      " [0.57109061 0.00496354 0.06286685 0.36107901]\n",
      " [0.06113076 0.02622561 0.7271175  0.18552614]\n",
      " [0.37039418 0.55187999 0.04810121 0.02962462]\n",
      " [0.29227444 0.09671244 0.42429411 0.186719  ]]\n",
      "score : 0.0009176737676124258\n",
      "---constraint_score : 1.150867763930057\n",
      "---objective_score : -1.1499500901624446\n",
      "score : 2.668020051957397\n",
      "---constraint_score : 3.803439201711381\n",
      "---objective_score : -1.135419149753984\n",
      "score : 5.0212713951843195\n",
      "---constraint_score : 6.833684518556421\n",
      "---objective_score : -1.812413123372101\n",
      "score : 0.5053463710598358\n",
      "---constraint_score : 2.1681287041890025\n",
      "---objective_score : -1.6627823331291667\n",
      "score : 7.638688956276907\n",
      "---constraint_score : 9.51340951492235\n",
      "---objective_score : -1.8747205586454427\n",
      "score : -0.187210329445877\n",
      "---constraint_score : 0.9914899901049555\n",
      "---objective_score : -1.1787003195508325\n",
      "score : 997.7755960103196\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -2.2244039896803782\n",
      "score : 0.20015288820095667\n",
      "---constraint_score : 1.2531246786476389\n",
      "---objective_score : -1.0529717904466822\n",
      "score : 998.9553094987923\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -1.0446905012077268\n",
      "score : 998.6646974996738\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -1.3353025003261936\n",
      "score : 6.850363241532787\n",
      "---constraint_score : 8.346173693979646\n",
      "---objective_score : -1.4958104524468585\n",
      "score : 1.7432080387653706\n",
      "---constraint_score : 2.918156084503226\n",
      "---objective_score : -1.1749480457378554\n",
      "score : 998.4467811468351\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -1.5532188531649125\n",
      "score : 0.20308365452561272\n",
      "---constraint_score : 1.0228488519070407\n",
      "---objective_score : -0.819765197381428\n",
      "score : 998.2032695841167\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -1.796730415883275\n",
      "score : -0.2023531398107895\n",
      "---constraint_score : 0.899748006954635\n",
      "---objective_score : -1.1021011467654245\n",
      "score : 0.07110963038107498\n",
      "---constraint_score : 1.1447042600757873\n",
      "---objective_score : -1.0735946296947123\n"
     ]
    }
   ],
   "source": [
    "trained_es = inv_barrier_constrained_explore(target_performance, num_train_intervals)\n",
    "new_policy = policy_softmax(trained_es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_Convergence : 150.313497890791\n"
     ]
    }
   ],
   "source": [
    "print(\"ES_Convergence : \" + str(sum(trained_es.mean**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Final Results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: -0.8479867413873314\n",
      "Predicted Baseline: -0.6104487121582425\n",
      "Safety Baseline: 0.4380582971385134\n",
      "---distance of return from lower bounds ---\n",
      "Looseness Of Prediction : 0.23753802922908895\n",
      "Looseness Of Safety : 1.2860450385258448\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ConfirmBounds(False, target_performance, train, test, exploration_policy, gamma, new_policy, ISFunc, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_safety_lower_bound = Safety_Test(test, exploration_policy, gamma, new_policy, ISFunc, delta)\n",
    "SavePolicy(new_policy, J_safety_lower_bound, delta, USE_GRIDWORLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  2  2  3]\n",
      " [ 1  1  1  3  3]\n",
      " [ 0  1 -1  2  2]\n",
      " [ 1  0 -1  3  2]\n",
      " [ 2  0  2  1  0]]\n"
     ]
    }
   ],
   "source": [
    "greed_policy = np.argmax(new_policy,axis=1)\n",
    "\n",
    "if(USE_GRIDWORLD):\n",
    "    print(np.insert(greed_policy, [12,16],[-1,-1]).reshape((5,5)))\n",
    "else:\n",
    "    print(greed_policy.reshape(6,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_Convergence : 150.313497890791\n"
     ]
    }
   ],
   "source": [
    "pickle_name = \"pickles\\\\saved-cma-\" + (\"gw\" if USE_GRIDWORLD else \"van\") + \"-\" + (\"pdis\" if USE_PDIS else \"is\") + \"-\" + str(num_train_intervals) + \".pkl\"\n",
    "pickle.dump(trained_es, open(pickle_name, \"wb\"))\n",
    "es = pickle.load(open(pickle_name, \"rb\"))\n",
    "print(\"ES_Convergence : \" + str(sum(es.mean**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
