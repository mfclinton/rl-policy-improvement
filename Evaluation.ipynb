{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from lib.DataManager import *\n",
    "from lib.PolicyStats import *\n",
    "import os\n",
    "import cma\n",
    "from cma.constraints_handler import AugmentedLagrangian, PopulationEvaluator\n",
    "from IPython import display\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET PARAMS---\n",
    "USE_GRIDWORLD = False\n",
    "USE_PDIS = True\n",
    "num_train_intervals = 50\n",
    "percent_increase = 0.1\n",
    "\n",
    "num_states = 18\n",
    "if(USE_GRIDWORLD):\n",
    "    num_states = 23\n",
    "num_actions = 4\n",
    "gamma = 0.95\n",
    "delta = 0.01 #1 - delta, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 0\n",
      "line 1000000\n",
      "line 2000000\n",
      "line 3000000\n",
      "line 4000000\n",
      "line 5000000\n",
      "line 6000000\n",
      "line 7000000\n",
      "line 8000000\n",
      "line 9000000\n",
      "line 10000000\n",
      "line 11000000\n",
      "line 12000000\n",
      "line 13000000\n",
      "line 14000000\n",
      "line 15000000\n",
      "line 16000000\n",
      "line 17000000\n",
      "line 18000000\n",
      "line 19000000\n",
      "line 20000000\n",
      "line 21000000\n",
      "line 22000000\n",
      "line 23000000\n",
      "line 24000000\n",
      "line 25000000\n",
      "line 26000000\n",
      "line 27000000\n",
      "line 28000000\n",
      "line 29000000\n",
      "line 30000000\n",
      "line 31000000\n",
      "line 32000000\n",
      "line 33000000\n",
      "line 34000000\n",
      "line 35000000\n",
      "line 36000000\n",
      "line 37000000\n",
      "line 38000000\n",
      "line 39000000\n",
      "line 40000000\n",
      "line 41000000\n",
      "line 42000000\n",
      "line 43000000\n",
      "line 44000000\n",
      "line 45000000\n",
      "line 46000000\n",
      "line 47000000\n",
      "line 48000000\n",
      "line 49000000\n",
      "line 50000000\n",
      "line 51000000\n",
      "line 52000000\n",
      "line 53000000\n",
      "line 54000000\n",
      "line 55000000\n",
      "line 56000000\n",
      "line 57000000\n",
      "line 58000000\n",
      "line 59000000\n",
      "line 60000000\n",
      "line 61000000\n",
      "line 62000000\n"
     ]
    }
   ],
   "source": [
    "path = \"data\\data.csv\"\n",
    "if(USE_GRIDWORLD):\n",
    "    path = \"data\\gridworld_data.csv\"\n",
    "\n",
    "histories = GetHistories(path, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Baseline Return : 1.172600237817154\n"
     ]
    }
   ],
   "source": [
    "avg_exploratory_J = GetAverageReturn(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set Target***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Performance : 1.556907\n"
     ]
    }
   ],
   "source": [
    "target_performance = GetTargetPerformance(USE_GRIDWORLD, avg_exploratory_J, percent_increase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Split Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "train, test = SplitData(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Exploration Policy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.25  0.25  0.25  0.25 ]\n",
      " [0.375 0.375 0.125 0.125]]\n"
     ]
    }
   ],
   "source": [
    "exploration_policy = GetPolicy(train, num_states, num_actions, 1000)\n",
    "print(exploration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"exploration_policy.npy\", exploration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pick Importance Sampling Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISFunc = ImportanceSampling\n",
    "if(USE_PDIS):\n",
    "    ISFunc = PDImportanceSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluate Current Policy On Candidate/Safety Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 1.172600237817154\n",
      "Predicted Baseline: 1.1556262108490003\n",
      "Safety Baseline: 1.1682832178181233\n",
      "---distance of return from lower bounds ---\n",
      "Looseness Of Prediction : 0.016974026968153755\n",
      "Looseness Of Safety : 0.00431701999903078\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ConfirmBounds(True, avg_exploratory_J, train, test, exploration_policy, gamma, exploration_policy, ISFunc, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Helper Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_softmax(policy):\n",
    "    numerators = np.exp(policy)\n",
    "    return (numerators.T / np.sum(numerators, axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sucks, never use it lol\n",
    "def random_explore():\n",
    "    best_policy = exploration_policy.copy()\n",
    "    max_lower_bound = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        random_step = np.random.normal(0, 1, best_policy.shape)\n",
    "        new_policy = policy_softmax(best_policy + random_step)\n",
    "\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test))\n",
    "        print(\"Predicted Lower Bound: \", J_predicted_lower_bound)\n",
    "        if(J_predicted_lower_bound > max_lower_bound):\n",
    "            print(\"Policy Updated\")\n",
    "            best_policy = new_policy\n",
    "            max_lower_bound = J_predicted_lower_bound\n",
    "        print(\"---------------\")\n",
    "\n",
    "    print(best_policy)\n",
    "    return best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also sucks, use the constrained variant\n",
    "def unconstrained_explore():\n",
    "    def objective(s):\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        print(avgIS)\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while not es.stop():\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [objective(s) for s in solutions])\n",
    "        \n",
    "    return policy_softmax(es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_barrier_constrained_explore(lower_bound_goal, max_updates=10):\n",
    "    #Helper Functions\n",
    "    #This constraint makes sure our results are passing the safety prediction test\n",
    "    def constraint(new_policy, avgIS):\n",
    "        EPSILON = 0.001 #determines penalty for failing lower bound test\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test), avgIS)\n",
    "        return 1 / (max(J_predicted_lower_bound - lower_bound_goal, EPSILON)) #TODO validate constraint\n",
    "    \n",
    "    #This objective results in maximizing the average importance sampling\n",
    "    def objective(new_policy, avgIS):\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    def optimizing_function(s):\n",
    "        #softmax generated policy\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        \n",
    "        #caches the averageIS so we don't have to recompute\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        \n",
    "        #computes score from the objective and constraint\n",
    "        objective_score = objective(new_policy, avgIS)\n",
    "        constraint_score = constraint(new_policy, avgIS)\n",
    "        score = objective_score + constraint_score\n",
    "        print(\"score : \" + str(score) + \"\\n---constraint_score : \" + str(constraint_score) + \"\\n---objective_score : \" + str(objective_score))\n",
    "        return score\n",
    "    \n",
    "    i = 0\n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while (not es.stop() and i != max_updates):\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(\"Update : \" + str(i))\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [optimizing_function(s) for s in solutions])\n",
    "        i += 1\n",
    "        \n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explore Policies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update : 0\n",
      "[[0.11873524 0.30342526 0.2169124  0.3609271 ]\n",
      " [0.31807093 0.30133512 0.2793935  0.10120045]\n",
      " [0.15917271 0.4099914  0.31147731 0.11935858]\n",
      " [0.1661041  0.09644574 0.12767748 0.60977268]\n",
      " [0.18131523 0.24420533 0.46135757 0.11312187]\n",
      " [0.33011637 0.30042842 0.15815074 0.21130447]\n",
      " [0.23128722 0.11813172 0.32221375 0.32836731]\n",
      " [0.29721434 0.14594431 0.40024104 0.15660031]\n",
      " [0.14084254 0.14172576 0.31364237 0.40378932]\n",
      " [0.24262718 0.27439315 0.36441419 0.11856547]\n",
      " [0.12472247 0.39374767 0.12332225 0.35820762]\n",
      " [0.58766564 0.10989728 0.1191396  0.18329749]\n",
      " [0.17372856 0.29294101 0.35486571 0.17846472]\n",
      " [0.26003114 0.15172827 0.44539932 0.14284127]\n",
      " [0.19692554 0.28900267 0.29847441 0.21559738]\n",
      " [0.21403272 0.40145988 0.25299075 0.13151665]\n",
      " [0.18362686 0.25810009 0.37643507 0.18183798]\n",
      " [0.33668036 0.14095224 0.3328288  0.1895386 ]]\n",
      "score : 999.0325050619156\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.9674949380844687\n"
     ]
    }
   ],
   "source": [
    "trained_es = inv_barrier_constrained_explore(target_performance, num_train_intervals)\n",
    "new_policy = policy_softmax(trained_es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ES_Convergence : \" + str(sum(trained_es.mean**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Final Results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfirmBounds(False, target_performance, train, test, exploration_policy, gamma, new_policy, ISFunc, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_safety_lower_bound = Safety_Test(test, exploration_policy, gamma, new_policy, ISFunc, delta)\n",
    "SavePolicy(new_policy, J_safety_lower_bound, delta, USE_GRIDWORLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greed_policy = np.argmax(new_policy,axis=1)\n",
    "\n",
    "if(USE_GRIDWORLD):\n",
    "    print(np.insert(greed_policy, [12,16],[-1,-1]).reshape((5,5)))\n",
    "else:\n",
    "    print(greed_policy.reshape(6,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = \"pickles\\\\saved-cma-\" + (\"gw\" if USE_GRIDWORLD else \"van\") + \"-\" + (\"pdis\" if USE_PDIS else \"is\") + \"-\" + str(num_train_intervals) + \".pkl\"\n",
    "pickle.dump(trained_es, open(pickle_name, \"wb\"))\n",
    "es = pickle.load(open(pickle_name, \"rb\"))\n",
    "print(\"ES_Convergence : \" + str(sum(es.mean**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
