{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from lib.DataManager import *\n",
    "from lib.PolicyStats import *\n",
    "import os\n",
    "import cma\n",
    "from cma.constraints_handler import AugmentedLagrangian, PopulationEvaluator\n",
    "from IPython import display\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET PARAMS---\n",
    "USE_GRIDWORLD = True\n",
    "USE_PDIS = False\n",
    "num_train_intervals = 10\n",
    "percent_increase = 0.1\n",
    "\n",
    "num_states = 18\n",
    "if(USE_GRIDWORLD):\n",
    "    num_states = 23\n",
    "num_actions = 4\n",
    "gamma = 0.95\n",
    "delta = 0.01 #1 - delta, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 0\n",
      "line 1000000\n",
      "line 2000000\n",
      "line 3000000\n",
      "line 4000000\n",
      "line 5000000\n",
      "line 6000000\n",
      "line 7000000\n"
     ]
    }
   ],
   "source": [
    "path = \"data\\data.csv\"\n",
    "if(USE_GRIDWORLD):\n",
    "    path = \"data\\gridworld_data.csv\"\n",
    "\n",
    "histories = GetHistories(path, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Baseline Return : -0.9422074904303682\n"
     ]
    }
   ],
   "source": [
    "avg_exploratory_J = 0\n",
    "for traj in histories:\n",
    "    avg_exploratory_J += traj[\"return\"]\n",
    "    \n",
    "avg_exploratory_J /= len(histories)\n",
    "print(\"Average Baseline Return : \" + str(avg_exploratory_J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set Target***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Performance : -0.8479867413873314\n"
     ]
    }
   ],
   "source": [
    "target_performance = 1.41537\n",
    "if(USE_GRIDWORLD):\n",
    "    target_performance = avg_exploratory_J\n",
    "    \n",
    "target_performance += abs(target_performance)*percent_increase #% increase\n",
    "print(\"Target Performance : \" + str(target_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Split Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(histories) * .8)\n",
    "train = histories[:split_idx]\n",
    "test = histories[split_idx:]\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Exploration Policy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]\n",
      " [0.25 0.25 0.25 0.25]]\n"
     ]
    }
   ],
   "source": [
    "exploration_policy = GetPolicy(train, num_states, num_actions, 1000)\n",
    "print(exploration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pick Importance Sampling Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISFunc = ImportanceSampling\n",
    "if(USE_PDIS):\n",
    "    ISFunc = PDImportanceSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluate Current Policy On Candidate/Safety Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Baseline: -1.0242265270774407\n",
      "Safety Baseline: -0.9720581231722768\n",
      "---distance of return from lower bounds ---\n",
      "Looseness Of Prediction : 0.08201903664707244\n",
      "Looseness Of Safety : 0.029850632741908578\n"
     ]
    }
   ],
   "source": [
    "ConfirmBounds(True, avg_exploratory_J, train, test, exploration_policy, gamma, exploration_policy, ISFunc, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Helper Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_softmax(policy):\n",
    "    numerators = np.exp(policy)\n",
    "    return (numerators.T / np.sum(numerators, axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sucks, never use it lol\n",
    "def random_explore():\n",
    "    best_policy = exploration_policy.copy()\n",
    "    max_lower_bound = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        random_step = np.random.normal(0, 1, best_policy.shape)\n",
    "        new_policy = policy_softmax(best_policy + random_step)\n",
    "\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test))\n",
    "        print(\"Predicted Lower Bound: \", J_predicted_lower_bound)\n",
    "        if(J_predicted_lower_bound > max_lower_bound):\n",
    "            print(\"Policy Updated\")\n",
    "            best_policy = new_policy\n",
    "            max_lower_bound = J_predicted_lower_bound\n",
    "        print(\"---------------\")\n",
    "\n",
    "    print(best_policy)\n",
    "    return best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also sucks, use the constrained variant\n",
    "def unconstrained_explore():\n",
    "    def objective(s):\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        print(avgIS)\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while not es.stop():\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [objective(s) for s in solutions])\n",
    "        \n",
    "    return policy_softmax(es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_barrier_constrained_explore(lower_bound_goal, max_updates=10):\n",
    "    #Helper Functions\n",
    "    #This constraint makes sure our results are passing the safety prediction test\n",
    "    def constraint(new_policy, avgIS):\n",
    "        EPSILON = 0.001 #determines penalty for failing lower bound test\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test), avgIS)\n",
    "        return 1 / (max(J_predicted_lower_bound - lower_bound_goal, EPSILON))\n",
    "    \n",
    "    #This objective results in maximizing the average importance sampling\n",
    "    def objective(new_policy, avgIS):\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    def optimizing_function(s):\n",
    "        #softmax generated policy\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        \n",
    "        #caches the averageIS so we don't have to recompute\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        \n",
    "        #computes score from the objective and constraint\n",
    "        objective_score = objective(new_policy, avgIS)\n",
    "        constraint_score = constraint(new_policy, avgIS)\n",
    "        score = objective_score + constraint_score\n",
    "        print(\"score : \" + str(score) + \"\\n---constraint_score : \" + str(constraint_score) + \"\\n---objective_score : \" + str(objective_score))\n",
    "        return score\n",
    "    \n",
    "    i = 0\n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while (not es.stop() and i != max_updates):\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(\"Update : \" + str(i))\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [optimizing_function(s) for s in solutions])\n",
    "        i += 1\n",
    "        \n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explore Policies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update : 9\n",
      "[[0.24756525 0.11091496 0.20831065 0.43320913]\n",
      " [0.07023039 0.23384795 0.04396644 0.65195522]\n",
      " [0.4735708  0.38978473 0.07069771 0.06594676]\n",
      " [0.4431821  0.33396297 0.12601004 0.09684489]\n",
      " [0.07206047 0.7327267  0.08740366 0.10780917]\n",
      " [0.67690333 0.13419453 0.15797351 0.03092863]\n",
      " [0.1250434  0.35260408 0.42520694 0.09714558]\n",
      " [0.65756555 0.05121622 0.10691529 0.18430294]\n",
      " [0.16299782 0.28168684 0.29759491 0.25772044]\n",
      " [0.08702494 0.46044608 0.17402696 0.27850201]\n",
      " [0.12092118 0.31753348 0.06011329 0.50143206]\n",
      " [0.12477153 0.34152171 0.39106637 0.14264039]\n",
      " [0.26772484 0.38139972 0.16871681 0.18215863]\n",
      " [0.53616967 0.31689048 0.11808321 0.02885664]\n",
      " [0.42635483 0.13966308 0.32775126 0.10623083]\n",
      " [0.36428052 0.20463437 0.19638667 0.23469844]\n",
      " [0.42748939 0.28395662 0.17314548 0.11540851]\n",
      " [0.23347382 0.42203264 0.22728923 0.11720431]\n",
      " [0.06621269 0.12632012 0.5173068  0.29016039]\n",
      " [0.45653394 0.02542966 0.1169286  0.4011078 ]\n",
      " [0.48354948 0.23686277 0.11129244 0.16829531]\n",
      " [0.36259484 0.11329001 0.2650785  0.25903664]\n",
      " [0.15770633 0.3685463  0.11277043 0.36097694]]\n",
      "score : 1.2670868682200211\n",
      "---constraint_score : 1.2622124273371764\n",
      "---objective_score : 0.004874440882844659\n",
      "score : 1.2269981298113082\n",
      "---constraint_score : 1.2213834762914435\n",
      "---objective_score : 0.005614653519864618\n",
      "score : 1.3976992261522765\n",
      "---constraint_score : 1.3932508957776217\n",
      "---objective_score : 0.004448330374654808\n",
      "score : 1.3148068654856901\n",
      "---constraint_score : 1.3659574503060739\n",
      "---objective_score : -0.05115058482038374\n",
      "score : 1.4234339188677947\n",
      "---constraint_score : 1.4113636479911458\n",
      "---objective_score : 0.012070270876648964\n",
      "score : 1.3503116852772066\n",
      "---constraint_score : 1.3901542298551592\n",
      "---objective_score : -0.03984254457795261\n",
      "score : 1.2012550274462177\n",
      "---constraint_score : 1.2161125947332125\n",
      "---objective_score : -0.014857567286994888\n",
      "score : 1.2097103692216349\n",
      "---constraint_score : 1.2232204827010489\n",
      "---objective_score : -0.013510113479413922\n",
      "score : 1.2993837441780023\n",
      "---constraint_score : 1.3106173190176382\n",
      "---objective_score : -0.011233574839635994\n",
      "score : 1.4691408665458197\n",
      "---constraint_score : 1.4531809472134047\n",
      "---objective_score : 0.015959919332414887\n",
      "score : 1.7496035881858347\n",
      "---constraint_score : 1.7716224096514586\n",
      "---objective_score : -0.022018821465623728\n",
      "score : 1.2300869910827807\n",
      "---constraint_score : 1.230756262413165\n",
      "---objective_score : -0.0006692713303841839\n",
      "score : 1.1988563115273947\n",
      "---constraint_score : 1.2112675050912152\n",
      "---objective_score : -0.01241119356382053\n",
      "score : 1.8445779229594041\n",
      "---constraint_score : 1.8917247280740852\n",
      "---objective_score : -0.047146805114681056\n",
      "score : 1.2668097144285226\n",
      "---constraint_score : 1.2706916518497129\n",
      "---objective_score : -0.00388193742119035\n",
      "score : 1.5162767333780818\n",
      "---constraint_score : 1.5523256643204588\n",
      "---objective_score : -0.0360489309423771\n",
      "score : 1.2511971797756496\n",
      "---constraint_score : 1.2527024921647927\n",
      "---objective_score : -0.0015053123891431578\n"
     ]
    }
   ],
   "source": [
    "trained_es = inv_barrier_constrained_explore(target_performance, num_train_intervals)\n",
    "new_policy = policy_softmax(trained_es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_Convergence : 50.569243338700026\n"
     ]
    }
   ],
   "source": [
    "print(\"ES_Convergence : \" + str(sum(trained_es.mean**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Final Results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: -0.8479867413873314\n",
      "Predicted Baseline: -0.1614165548185133\n",
      "Safety Baseline: 0.014594610851782888\n",
      "---distance of return from lower bounds ---\n",
      "Looseness Of Prediction : 0.6865701865688181\n",
      "Looseness Of Safety : 0.8625813522391144\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ConfirmBounds(False, target_performance, train, test, exploration_policy, gamma, new_policy, ISFunc, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_safety_lower_bound = Safety_Test(test, exploration_policy, gamma, new_policy, ISFunc, delta)\n",
    "SavePolicy(new_policy, J_safety_lower_bound, delta, USE_GRIDWORLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  3  1  3  1]\n",
      " [ 0  1  0  2  1]\n",
      " [ 0  1 -1  1  2]\n",
      " [ 2  0 -1  1  1]\n",
      " [ 1  0  3  0  3]]\n"
     ]
    }
   ],
   "source": [
    "greed_policy = np.argmax(new_policy,axis=1)\n",
    "\n",
    "if(USE_GRIDWORLD):\n",
    "    print(np.insert(greed_policy, [12,16],[-1,-1]).reshape((5,5)))\n",
    "else:\n",
    "    print(greed_policy.reshape(6,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = \"pickles\\\\saved-cma-\" + (\"gw\" if USE_GRIDWORLD else \"van\") + \"-\" + (\"pdis\" if USE_PDIS else \"is\") + \"-\" + str(num_train_intervals) + \".pkl\"\n",
    "pickle.dump(trained_es, open(pickle_name, \"wb\"))\n",
    "es = pickle.load(open(pickle_name, \"rb\"))\n",
    "print(\"ES_Convergence : \" + str(sum(es.mean**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
