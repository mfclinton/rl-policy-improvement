{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from lib.DataManager import *\n",
    "from lib.PolicyStats import *\n",
    "import os\n",
    "import cma\n",
    "from cma.constraints_handler import AugmentedLagrangian, PopulationEvaluator\n",
    "from IPython import display\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET PARAMS---\n",
    "USE_GRIDWORLD = True\n",
    "USE_PDIS = True\n",
    "num_train_intervals = 10\n",
    "percent_increase = 0.01\n",
    "\n",
    "num_states = 18\n",
    "if(USE_GRIDWORLD):\n",
    "    num_states = 23\n",
    "num_actions = 4\n",
    "gamma = 0.95\n",
    "delta = 0.01 #1 - delta, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 0\n",
      "line 1000000\n",
      "line 2000000\n",
      "line 3000000\n"
     ]
    }
   ],
   "source": [
    "path = \"data\\data.csv\"\n",
    "if(USE_GRIDWORLD):\n",
    "    path = \"data\\gridworld_data.csv\"\n",
    "    path = \"data\\gridworld_data_0.6097323533319994.csv\"\n",
    "\n",
    "histories = GetHistories(path, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Baseline Return : 1.1918778099059908\n"
     ]
    }
   ],
   "source": [
    "avg_exploratory_J = 0\n",
    "for traj in histories:\n",
    "    avg_exploratory_J += traj[\"return\"]\n",
    "    \n",
    "avg_exploratory_J /= len(histories)\n",
    "print(\"Average Baseline Return : \" + str(avg_exploratory_J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set Target***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Performance : 1.2037965880050507\n"
     ]
    }
   ],
   "source": [
    "target_performance = 1.41537\n",
    "if(USE_GRIDWORLD):\n",
    "    target_performance = avg_exploratory_J\n",
    "    \n",
    "target_performance += abs(target_performance)*percent_increase #% increase\n",
    "print(\"Target Performance : \" + str(target_performance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Split Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(len(histories) * .8)\n",
    "train = histories[:split_idx]\n",
    "test = histories[split_idx:]\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Exploration Policy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11093975 0.67355548 0.08406882 0.13143595]\n",
      " [0.33845079 0.11190407 0.4565064  0.09313874]\n",
      " [0.08092026 0.47658339 0.38928016 0.05321619]\n",
      " [0.26997617 0.16391482 0.4399108  0.12619821]\n",
      " [0.40491451 0.27070029 0.13888351 0.18550169]\n",
      " [0.26437797 0.24798767 0.09878455 0.38884981]\n",
      " [0.28373503 0.60533749 0.09821795 0.01270952]\n",
      " [0.08435094 0.5817363  0.24638354 0.08752922]\n",
      " [0.05501383 0.11421107 0.79897272 0.03180237]\n",
      " [0.19865733 0.05400194 0.35845996 0.38888077]\n",
      " [0.05544299 0.12379174 0.08442272 0.73634255]\n",
      " [0.47737619 0.05951284 0.03469548 0.4284155 ]\n",
      " [0.07710719 0.50252871 0.18859377 0.23177033]\n",
      " [0.15065158 0.10532256 0.52729108 0.21673478]\n",
      " [0.16824125 0.03899513 0.06917945 0.72358417]\n",
      " [0.32641138 0.24798224 0.19835508 0.2272513 ]\n",
      " [0.10837872 0.51283487 0.29642909 0.08235732]\n",
      " [0.13401538 0.135192   0.4548163  0.27597632]\n",
      " [0.12690596 0.25388157 0.33273322 0.28647925]\n",
      " [0.34659794 0.07205988 0.44071458 0.14062761]\n",
      " [0.11738858 0.18007116 0.12818093 0.57435933]\n",
      " [0.40229665 0.29902799 0.22965748 0.06901789]\n",
      " [0.13896305 0.29981294 0.26868002 0.29254398]]\n"
     ]
    }
   ],
   "source": [
    "exploration_policy = GetPolicy(train, num_states, num_actions, 1000)\n",
    "print(exploration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pick Importance Sampling Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISFunc = ImportanceSampling\n",
    "if(USE_PDIS):\n",
    "    ISFunc = PDImportanceSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluate Current Policy On Candidate/Safety Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Baseline: 1.1449347893199329\n",
      "Safety Baseline: 1.1555727027611657\n",
      "---distance of average return from lower bounds ---\n",
      "Looseness Of Prediction : 0.046943020586057926\n",
      "Looseness Of Safety : 0.036305107144825044\n"
     ]
    }
   ],
   "source": [
    "# P(Je > J_lower_bound) > 1 - delta\n",
    "J_bl_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, exploration_policy, ISFunc, delta, len(test))\n",
    "print(\"Predicted Baseline: \" + str(J_bl_predicted_lower_bound))\n",
    "J_bl_safety_lower_bound = Safety_Test(test, exploration_policy, gamma, exploration_policy, ISFunc, delta)\n",
    "print(\"Safety Baseline: \" + str(J_bl_safety_lower_bound))\n",
    "\n",
    "# Ensures Lower Bound Is Lower, Otherwise Investigate\n",
    "bl_pred_looseness = avg_exploratory_J - J_bl_predicted_lower_bound\n",
    "bl_safety_looseness = avg_exploratory_J - J_bl_safety_lower_bound\n",
    "print(\"---distance of average return from lower bounds ---\")\n",
    "print(\"Looseness Of Prediction : \" + str(bl_pred_looseness))\n",
    "print(\"Looseness Of Safety : \" + str(bl_safety_looseness))\n",
    "\n",
    "if((bl_pred_looseness < 0) or (bl_safety_looseness < 0)):\n",
    "    raise Exception(\"Lower Bound Greater Than Average Return!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Helper Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_softmax(policy):\n",
    "    numerators = np.exp(policy)\n",
    "    return (numerators.T / np.sum(numerators, axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sucks, never use it lol\n",
    "def random_explore():\n",
    "    best_policy = exploration_policy.copy()\n",
    "    max_lower_bound = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        random_step = np.random.normal(0, 1, best_policy.shape)\n",
    "        new_policy = policy_softmax(best_policy + random_step)\n",
    "\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test))\n",
    "        print(\"Predicted Lower Bound: \", J_predicted_lower_bound)\n",
    "        if(J_predicted_lower_bound > max_lower_bound):\n",
    "            print(\"Policy Updated\")\n",
    "            best_policy = new_policy\n",
    "            max_lower_bound = J_predicted_lower_bound\n",
    "        print(\"---------------\")\n",
    "\n",
    "    print(best_policy)\n",
    "    return best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also sucks, use the constrained variant\n",
    "def unconstrained_explore():\n",
    "    def objective(s):\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        print(avgIS)\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while not es.stop():\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [objective(s) for s in solutions])\n",
    "        \n",
    "    return policy_softmax(es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_barrier_constrained_explore(lower_bound_goal, max_updates=100):\n",
    "    #Helper Functions\n",
    "    #This constraint makes sure our results are passing the safety prediction test\n",
    "    def constraint(new_policy, avgIS):\n",
    "        EPSILON = 0.001 #determines penalty for failing lower bound test\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test), avgIS)\n",
    "        return 1 / (max(J_predicted_lower_bound - lower_bound_goal, EPSILON))\n",
    "    \n",
    "    #This objective results in maximizing the average importance sampling\n",
    "    def objective(new_policy, avgIS):\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    def optimizing_function(s):\n",
    "        #softmax generated policy\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        \n",
    "        #caches the averageIS so we don't have to recompute\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        \n",
    "        #computes score from the objective and constraint\n",
    "        objective_score = objective(new_policy, avgIS)\n",
    "        constraint_score = constraint(new_policy, avgIS)\n",
    "        score = objective_score + constraint_score\n",
    "        print(\"score : \" + str(score) + \"\\n---constraint_score : \" + str(constraint_score) + \"\\n---objective_score : \" + str(objective_score))\n",
    "        return score\n",
    "    \n",
    "    i = 0\n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while (not es.stop() and i != max_updates):\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(\"Update : \" + str(i))\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [optimizing_function(s) for s in solutions])\n",
    "        i += 1\n",
    "        \n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explore Policies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update : 8\n",
      "[[0.16486415 0.48444028 0.19319869 0.15749688]\n",
      " [0.16231031 0.70364011 0.04623684 0.08781273]\n",
      " [0.13514506 0.12581907 0.59632964 0.14270624]\n",
      " [0.11905269 0.19701756 0.47018401 0.21374574]\n",
      " [0.06289964 0.1074163  0.29499863 0.53468543]\n",
      " [0.15074364 0.77778623 0.02605973 0.04541039]\n",
      " [0.45429758 0.14892127 0.10020478 0.29657638]\n",
      " [0.05742512 0.62951193 0.22523862 0.08782433]\n",
      " [0.36930494 0.11628769 0.45917816 0.05522921]\n",
      " [0.12307883 0.38298684 0.37478353 0.1191508 ]\n",
      " [0.13662095 0.24051489 0.22358594 0.39927822]\n",
      " [0.41671289 0.13322671 0.19127511 0.25878529]\n",
      " [0.27104674 0.18112986 0.13028686 0.41753653]\n",
      " [0.07054639 0.09590947 0.81440752 0.01913662]\n",
      " [0.50120558 0.09408572 0.28427026 0.12043844]\n",
      " [0.24141672 0.27734899 0.12304342 0.35819088]\n",
      " [0.21500863 0.22932718 0.09883185 0.45683235]\n",
      " [0.18423939 0.09186722 0.56712498 0.15676841]\n",
      " [0.17358502 0.71634317 0.06651918 0.04355263]\n",
      " [0.38994248 0.02895826 0.31395276 0.2671465 ]\n",
      " [0.11876784 0.35236806 0.18134047 0.34752362]\n",
      " [0.12967741 0.29793755 0.20755412 0.36483092]\n",
      " [0.04876612 0.21546008 0.47409007 0.26168374]]\n",
      "score : 998.9792136793832\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -1.0207863206167982\n",
      "score : 999.399039878913\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.6009601210870641\n",
      "score : 999.3018960429798\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.6981039570201759\n",
      "score : 999.2064893419557\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.7935106580443144\n",
      "score : 999.1624151589439\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.8375848410560833\n",
      "score : 999.3500198047756\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.6499801952244321\n",
      "score : 999.4957648615173\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.5042351384827728\n",
      "score : 999.4450826293022\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.55491737069784\n",
      "score : 999.128224624453\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.8717753755470822\n",
      "score : 999.3489526262961\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.6510473737038781\n",
      "score : 999.4781423638448\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.5218576361552855\n",
      "score : 999.4932400684401\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.5067599315599378\n",
      "score : 999.4729987657701\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.5270012342299081\n",
      "score : 999.1978953564048\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.8021046435951074\n",
      "score : 999.3679662542075\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.6320337457924774\n",
      "score : 999.1774593620778\n",
      "---constraint_score : 1000.0\n",
      "---objective_score : -0.8225406379221455\n"
     ]
    }
   ],
   "source": [
    "trained_es = inv_barrier_constrained_explore(target_performance, num_train_intervals)\n",
    "new_policy = policy_softmax(trained_es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ES_Convergence : \" + str(sum(trained_es.mean**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Final Results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test))\n",
    "print(\"Predicted : \" + str(J_predicted_lower_bound))\n",
    "J_safety_lower_bound = Safety_Test(test, exploration_policy, gamma, new_policy, ISFunc, delta)\n",
    "print(\"Safety : \"  + str(J_safety_lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"policies\\\\delta_\" + str(delta) + \"\\\\\"\n",
    "if(USE_GRIDWORLD):\n",
    "    folder_name = \"policies\\\\gw\\\\delta_\" + str(delta) + \"\\\\\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "np.save(folder_name + \"safety_\" + str(J_safety_lower_bound), new_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greed_policy = np.argmax(new_policy,axis=1)\n",
    "\n",
    "if(USE_GRIDWORLD):\n",
    "    print(np.insert(greed_policy, [12,16],[-1,-1]).reshape((5,5)))\n",
    "else:\n",
    "    print(greed_policy.reshape(6,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = \"pickles\\\\saved-cma-\" + (\"gw\" if USE_GRIDWORLD else \"van\") + \"-\" + (\"pdis\" if USE_PDIS else \"is\") + \"-\" + str(num_train_intervals) + \".pkl\"\n",
    "pickle.dump(trained_es, open(pickle_name, \"wb\"))\n",
    "es = pickle.load(open(pickle_name, \"rb\"))\n",
    "print(\"ES_Convergence : \" + str(sum(es.mean**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
