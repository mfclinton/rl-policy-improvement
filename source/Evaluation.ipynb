{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from lib.DataManager import *\n",
    "from lib.PolicyStats import *\n",
    "import os\n",
    "import cma\n",
    "from cma.constraints_handler import AugmentedLagrangian, PopulationEvaluator\n",
    "from IPython import display\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "### Refer to ReadMe for how to set the seeds and the USE_PDIS parameter and num_train intervals\n",
    "### Other variables remain the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---SET PARAMS---\n",
    "USE_GRIDWORLD = False\n",
    "USE_PDIS = False\n",
    "num_train_intervals = 5\n",
    "percent_increase = 0.1\n",
    "gamma = 0.95\n",
    "delta = 0.01 #1 - delta, confidence\n",
    "seed = 768362 #see readme for what to set this as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loads world and establishes world states***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 18\n",
    "if(USE_GRIDWORLD):\n",
    "    num_states = 23\n",
    "num_actions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 0\n",
      "line 1000000\n",
      "line 2000000\n",
      "line 3000000\n",
      "line 4000000\n",
      "line 5000000\n",
      "line 6000000\n",
      "line 7000000\n",
      "line 8000000\n",
      "line 9000000\n",
      "line 10000000\n",
      "line 11000000\n",
      "line 12000000\n",
      "line 13000000\n",
      "line 14000000\n",
      "line 15000000\n",
      "line 16000000\n",
      "line 17000000\n",
      "line 18000000\n",
      "line 19000000\n",
      "line 20000000\n",
      "line 21000000\n",
      "line 22000000\n",
      "line 23000000\n",
      "line 24000000\n",
      "line 25000000\n"
     ]
    }
   ],
   "source": [
    "path = \"data\\data.csv\"\n",
    "if(USE_GRIDWORLD):\n",
    "    path = \"data\\gridworld_data.csv\"\n",
    "\n",
    "histories = GetHistories(path, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Data Analysis on Average Return of Exploratory Policy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_exploratory_J = GetAverageReturn(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set Target***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_performance = GetTargetPerformance(USE_GRIDWORLD, avg_exploratory_J, percent_increase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Split Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = SplitData(histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Get Exploration Policy***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_policy = GetPolicy(train, num_states, num_actions, 1000)\n",
    "print(exploration_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"exploration_policy.npy\", exploration_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Pick Importance Sampling Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ISFunc = ImportanceSampling\n",
    "if(USE_PDIS):\n",
    "    ISFunc = PDImportanceSampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Evaluate Current Policy On Candidate/Safety Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfirmBounds(True, avg_exploratory_J, train, test, exploration_policy, gamma, exploration_policy, ISFunc, delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Helper Functions***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_softmax(policy):\n",
    "    numerators = np.exp(policy)\n",
    "    return (numerators.T / np.sum(numerators, axis=1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sucks, never use it lol\n",
    "def random_explore():\n",
    "    best_policy = exploration_policy.copy()\n",
    "    max_lower_bound = 0\n",
    "\n",
    "    for i in range(100):\n",
    "        random_step = np.random.normal(0, 1, best_policy.shape)\n",
    "        new_policy = policy_softmax(best_policy + random_step)\n",
    "\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test))\n",
    "        print(\"Predicted Lower Bound: \", J_predicted_lower_bound)\n",
    "        if(J_predicted_lower_bound > max_lower_bound):\n",
    "            print(\"Policy Updated\")\n",
    "            best_policy = new_policy\n",
    "            max_lower_bound = J_predicted_lower_bound\n",
    "        print(\"---------------\")\n",
    "\n",
    "    print(best_policy)\n",
    "    return best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also sucks, use the constrained variant\n",
    "def unconstrained_explore():\n",
    "    def objective(s):\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        print(avgIS)\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.5)\n",
    "    while not es.stop():\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [objective(s) for s in solutions])\n",
    "        \n",
    "    return policy_softmax(es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_barrier_constrained_explore(lower_bound_goal, max_updates=10):\n",
    "    #Helper Functions\n",
    "    #This constraint makes sure our results are passing the safety prediction test\n",
    "    def constraint(new_policy, avgIS):\n",
    "        EPSILON = 0.001 #determines penalty for failing lower bound test\n",
    "        J_predicted_lower_bound = Safety_Prediction(train, exploration_policy, gamma, new_policy, ISFunc, delta, len(test), avgIS)\n",
    "        return 1 / (max(J_predicted_lower_bound - lower_bound_goal, EPSILON)) #TODO validate constraint\n",
    "    \n",
    "    #This objective results in maximizing the average importance sampling\n",
    "    def objective(new_policy, avgIS):\n",
    "        return - avgIS #minimizing\n",
    "    \n",
    "    def optimizing_function(s):\n",
    "        #softmax generated policy\n",
    "        new_policy = policy_softmax(s.reshape(num_states, num_actions))\n",
    "        \n",
    "        #caches the averageIS so we don't have to recompute\n",
    "        avgIS = CalcAvgIS(train, exploration_policy, gamma, new_policy, ISFunc)\n",
    "        \n",
    "        #computes score from the objective and constraint\n",
    "        objective_score = objective(new_policy, avgIS)\n",
    "        constraint_score = constraint(new_policy, avgIS)\n",
    "        score = objective_score + constraint_score\n",
    "        print(\"score : \" + str(score) + \"\\n---constraint_score : \" + str(constraint_score) + \"\\n---objective_score : \" + str(objective_score))\n",
    "        return score\n",
    "    \n",
    "    i = 0\n",
    "#     es = trained_es\n",
    "    es = cma.CMAEvolutionStrategy(num_states * num_actions * [0], 0.8, {'seed':seed})\n",
    "    while (not es.stop() and i != max_updates):\n",
    "        solutions = es.ask()\n",
    "        display.clear_output(True)\n",
    "        print(\"Update : \" + str(i))\n",
    "        print(policy_softmax(solutions[0].reshape(num_states, num_actions)))\n",
    "        es.tell(solutions, [optimizing_function(s) for s in solutions])\n",
    "        i += 1\n",
    "        \n",
    "    return es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Explore Policies***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_es = inv_barrier_constrained_explore(target_performance, num_train_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_name = \"pickles\\\\is_1-cma-\" + (\"gw\" if USE_GRIDWORLD else \"van\") + \"-\" + (\"pdis\" if USE_PDIS else \"is\") + \"-\" + str(num_train_intervals) + \".pkl\"\n",
    "pickle.dump(trained_es, open(pickle_name, \"wb\"))\n",
    "es = pickle.load(open(pickle_name, \"rb\"))\n",
    "print(\"ES_Convergence : \" + str(sum(es.mean**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_policy = policy_softmax(trained_es.ask()[0].reshape(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ES_Convergence : \" + str(sum(trained_es.mean**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Final Results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfirmBounds(False, target_performance, train, test, exploration_policy, gamma, new_policy, ISFunc, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J_safety_lower_bound = Safety_Test(test, exploration_policy, gamma, new_policy, ISFunc, delta)\n",
    "SaveNumpyPolicy(new_policy, J_safety_lower_bound, delta, USE_GRIDWORLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greed_policy = np.argmax(new_policy,axis=1)\n",
    "\n",
    "if(USE_GRIDWORLD):\n",
    "    print(np.insert(greed_policy, [12,16],[-1,-1]).reshape((5,5)))\n",
    "else:\n",
    "    print(greed_policy[:-2].reshape(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
